                      :-) GROMACS - gmx mdrun, 2019.1 (-:

                            GROMACS is written by:
     Emile Apol      Rossen Apostolov      Paul Bauer     Herman J.C. Berendsen
    Par Bjelkmar      Christian Blau   Viacheslav Bolnykh     Kevin Boyd    
 Aldert van Buuren   Rudi van Drunen     Anton Feenstra       Alan Gray     
  Gerrit Groenhof     Anca Hamuraru    Vincent Hindriksen  M. Eric Irrgang  
  Aleksei Iupinov   Christoph Junghans     Joe Jordan     Dimitrios Karkoulis
    Peter Kasson        Jiri Kraus      Carsten Kutzner      Per Larsson    
  Justin A. Lemkul    Viveca Lindahl    Magnus Lundborg     Erik Marklund   
    Pascal Merz     Pieter Meulenhoff    Teemu Murtola       Szilard Pall   
    Sander Pronk      Roland Schulz      Michael Shirts    Alexey Shvetsov  
   Alfons Sijbers     Peter Tieleman      Jon Vincent      Teemu Virolainen 
 Christian Wennberg    Maarten Wolf   
                           and the project leaders:
        Mark Abraham, Berk Hess, Erik Lindahl, and David van der Spoel

Copyright (c) 1991-2000, University of Groningen, The Netherlands.
Copyright (c) 2001-2018, The GROMACS development team at
Uppsala University, Stockholm University and
the Royal Institute of Technology, Sweden.
check out http://www.gromacs.org for more information.

GROMACS is free software; you can redistribute it and/or modify it
under the terms of the GNU Lesser General Public License
as published by the Free Software Foundation; either version 2.1
of the License, or (at your option) any later version.

GROMACS:      gmx mdrun, version 2019.1
Executable:   /Softs/lumiere/gromacs/mpi/gcc/2019.1-sp/bin/gmx_mpi
Data prefix:  /Softs/lumiere/gromacs/mpi/gcc/2019.1-sp
Working dir:  /Work/Users/ctsilefski/water
Command line:
  gmx_mpi mdrun -s topol.tpr


Back Off! I just backed up md.log to ./#md.log.7#
Compiled SIMD: AVX_256, but for this host/run AVX2_256 might be better (see
log).
Reading file topol.tpr, VERSION 2019.1 (single precision)
Using 16 MPI processes
Using 1 OpenMP thread per MPI process


Non-default thread affinity set probably by the OpenMP library,
disabling internal thread affinity

Back Off! I just backed up traj.trr to ./#traj.trr.3#

Back Off! I just backed up ener.edr to ./#ener.edr.3#

Steepest Descents:
   Tolerance (Fmax)   =  1.00000e+01
   Number of steps    =        10000

Energy minimization has stopped, but the forces have not converged to the
requested precision Fmax < 10 (which may not be possible for your system). It
stopped because the algorithm tried to make a new step whose size was too
small, or there was no change in the energy since last step. Either way, we
regard the minimization as converged to within the available machine
precision, given your starting configuration and EM parameters.

Double precision normally gives you higher accuracy, but this is often not
needed for preparing to run molecular dynamics.
You might need to increase your constraint accuracy, or turn
off constraints altogether (set constraints = none in mdp file)

writing lowest energy coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

Steepest Descents converged to machine precision in 1865 steps,
but did not reach the requested Fmax < 10.
Potential Energy  = -1.2915124e+05
Maximum force     =  3.8408352e+01 on atom 111
Norm of force     =  2.4133474e+01

GROMACS reminds you: "Computer science is no more about computers than astronomy is about telescopes" (Edsger Dijkstra)

