                      :-) GROMACS - gmx mdrun, 2019.1 (-:

                            GROMACS is written by:
     Emile Apol      Rossen Apostolov      Paul Bauer     Herman J.C. Berendsen
    Par Bjelkmar      Christian Blau   Viacheslav Bolnykh     Kevin Boyd    
 Aldert van Buuren   Rudi van Drunen     Anton Feenstra       Alan Gray     
  Gerrit Groenhof     Anca Hamuraru    Vincent Hindriksen  M. Eric Irrgang  
  Aleksei Iupinov   Christoph Junghans     Joe Jordan     Dimitrios Karkoulis
    Peter Kasson        Jiri Kraus      Carsten Kutzner      Per Larsson    
  Justin A. Lemkul    Viveca Lindahl    Magnus Lundborg     Erik Marklund   
    Pascal Merz     Pieter Meulenhoff    Teemu Murtola       Szilard Pall   
    Sander Pronk      Roland Schulz      Michael Shirts    Alexey Shvetsov  
   Alfons Sijbers     Peter Tieleman      Jon Vincent      Teemu Virolainen 
 Christian Wennberg    Maarten Wolf   
                           and the project leaders:
        Mark Abraham, Berk Hess, Erik Lindahl, and David van der Spoel

Copyright (c) 1991-2000, University of Groningen, The Netherlands.
Copyright (c) 2001-2018, The GROMACS development team at
Uppsala University, Stockholm University and
the Royal Institute of Technology, Sweden.
check out http://www.gromacs.org for more information.

GROMACS is free software; you can redistribute it and/or modify it
under the terms of the GNU Lesser General Public License
as published by the Free Software Foundation; either version 2.1
of the License, or (at your option) any later version.

GROMACS:      gmx mdrun, version 2019.1
Executable:   /Softs/lumiere/gromacs/mpi/gcc/2019.1-sp/bin/gmx_mpi
Data prefix:  /Softs/lumiere/gromacs/mpi/gcc/2019.1-sp
Working dir:  /Work/Users/ctsilefski/water
Command line:
  gmx_mpi mdrun -s topol.tpr


Back Off! I just backed up md.log to ./#md.log.5#
Compiled SIMD: AVX_256, but for this host/run AVX2_256 might be better (see
log).
Reading file topol.tpr, VERSION 2019.1 (single precision)
Changing nstlist from 10 to 50, rlist from 0.9 to 0.979

Using 16 MPI processes
Using 1 OpenMP thread per MPI process


Non-default thread affinity set probably by the OpenMP library,
disabling internal thread affinity

Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Water'
1000000 steps,   1000.0 ps.

-------------------------------------------------------
Program:     gmx mdrun, version 2019.1
Source file: src/gromacs/domdec/cellsizes.cpp (line 310)
MPI rank:    0 (out of 16)

Fatal error:
The box size in direction X (1.957976) times the triclinic skew factor
(1.000000) is too small for a cut-off of 0.979000 with 2 domain decomposition
cells, use 1 or more than 2 cells or increase the box size in this direction

For more information and tips for troubleshooting, please check the GROMACS
website at http://www.gromacs.org/Documentation/Errors
-------------------------------------------------------
-------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code.. Per user-direction, the job has been aborted.
-------------------------------------------------------
--------------------------------------------------------------------------
mpirun detected that one or more processes exited with non-zero status, thus causing
the job to be terminated. The first process to do so was:

  Process name: [[54783,1],1]
  Exit code:    1
--------------------------------------------------------------------------
